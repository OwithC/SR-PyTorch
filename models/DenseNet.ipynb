{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d298d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                \n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from PIL import ImageFile\n",
    "from torchvision import datasets\n",
    " \n",
    "batch_size = 128\n",
    "IMAGE_SIZE = 224\n",
    " \n",
    "MEANS = [0.485, 0.456, 0.406]\n",
    "DEVIATIONS = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "213f51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomRotation(30), # 이미지를 랜덤으로 30도 각도로 회전\n",
    "                                      transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.08, 1.0), ratio=(0.75, 1.33)), # 이미지 사이즈 변경\n",
    "                                      transforms.RandomHorizontalFlip(), # 이미지를 수평으로 뒤집는다\n",
    "                                      transforms.ToTensor(), # 데이터 타입을 Tensor 형태로 변경\n",
    "                                      transforms.Normalize(MEANS, DEVIATIONS) # 데이터 정규화\n",
    "                                      ])\n",
    " \n",
    "test_transform = transforms.Compose([transforms.Resize(IMAGE_SIZE), # IMAGE_SIZE로 이미지 사이즈 변경\n",
    "                                      transforms.CenterCrop(IMAGE_SIZE), # 이미지를 square하게 변경\n",
    "                                      transforms.ToTensor(), # 데이터 타입을 Tensor 형태로 변경\n",
    "                                      transforms.Normalize(MEANS, DEVIATIONS) # 데이터 정규화\n",
    "                                      ])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d7bb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "training = datasets.ImageFolder(\"./train\", transform=train_transform) # ImageFolder를 사용해 dataset 생성\n",
    "testing = datasets.ImageFolder(\"./test\", transform=test_transform) # ImageFolder를 사용해 dataset 생성\n",
    "\n",
    "train_batches = torch.utils.data.DataLoader(training, batch_size=batch_size, shuffle=True) # train_loader 생성\n",
    "test_batches = torch.utils.data.DataLoader(testing, batch_size=batch_size) # test 데이터는 batch사이즈 맞춰 그대로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de62ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer = models.densenet161(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f39435d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2208, out_features=5, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model_transfer.parameters():\n",
    "  param.requires_grad = False # requires_grad를 사용해 parameter 동결\n",
    " \n",
    "model_transfer.classifier = nn.Linear(model_transfer.classifier.in_features, 5) # 마지막 layer을 BREEDS개의 class로 분류하도록 재정의\n",
    "# 이 때, requires_grad 옵션은 True\n",
    " \n",
    "nn.init.kaiming_normal_(model_transfer.classifier.weight, nonlinearity='relu') # 재정의한 layer의 weight를 initialize\n",
    " \n",
    "print(model_transfer.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68149071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.2555\n",
      "Train loss decreased inf --> 1.2555. Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.0105\n",
      "Train loss decreased 1.2555 --> 1.0105. Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.9360\n",
      "Train loss decreased 1.0105 --> 0.9360. Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.9024\n",
      "Train loss decreased 0.9360 --> 0.9024. Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.8910\n",
      "Train loss decreased 0.9024 --> 0.8910. Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.8568\n",
      "Train loss decreased 0.8910 --> 0.8568. Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.8582\n",
      "Epoch: 8 \tTraining Loss: 0.8328\n",
      "Train loss decreased 0.8568 --> 0.8328. Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.8491\n",
      "Epoch: 10 \tTraining Loss: 0.8236\n",
      "Train loss decreased 0.8328 --> 0.8236. Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.8077\n",
      "Train loss decreased 0.8236 --> 0.8077. Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.8218\n",
      "Epoch: 13 \tTraining Loss: 0.8154\n",
      "Epoch: 14 \tTraining Loss: 0.7998\n",
      "Train loss decreased 0.8077 --> 0.7998. Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.7889\n",
      "Train loss decreased 0.7998 --> 0.7889. Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.7959\n",
      "Epoch: 17 \tTraining Loss: 0.8069\n",
      "Epoch: 18 \tTraining Loss: 0.7925\n",
      "Epoch: 19 \tTraining Loss: 0.7981\n",
      "Epoch: 20 \tTraining Loss: 0.7998\n",
      "Epoch: 21 \tTraining Loss: 0.7808\n",
      "Train loss decreased 0.7889 --> 0.7808. Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.7762\n",
      "Train loss decreased 0.7808 --> 0.7762. Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.7773\n",
      "Epoch: 24 \tTraining Loss: 0.7886\n",
      "Epoch: 25 \tTraining Loss: 0.7788\n",
      "Epoch: 26 \tTraining Loss: 0.7786\n",
      "Epoch: 27 \tTraining Loss: 0.7926\n",
      "Epoch: 28 \tTraining Loss: 0.7909\n",
      "Epoch: 29 \tTraining Loss: 0.7753\n",
      "Train loss decreased 0.7762 --> 0.7753. Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.7691\n",
      "Train loss decreased 0.7753 --> 0.7691. Saving model ...\n",
      "Test Loss: 0.805882\n",
      "\n",
      "\n",
      "Test Accuracy: 67% (1538/2270)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'myimg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8550ae91e284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'myimg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'myimg'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()\n",
    " \n",
    "criterion_transfer = nn.CrossEntropyLoss() \n",
    "optimizer_transfer = optim.Adam(model_transfer.parameters(), lr=0.001, betas=[0.9, 0.999])\n",
    " \n",
    "def train(n_epochs, train_loader, model, optimizer, criterion, use_cuda):  \n",
    "    train_losses = []\n",
    "    train_loss_min = np.Inf\n",
    " \n",
    "    for epoch in range(1, n_epochs+1): # epoch만큼 반복\n",
    "        # traing_loss와 valid_loss를 저장하기 위한 변수 선언\n",
    "        train_loss = 0.0\n",
    " \n",
    "        model.train() # 모델 training \n",
    "        for data, target in train_loader:\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda() # gpu에서 연산을 수행하기 위해\n",
    " \n",
    "            optimizer.zero_grad() # 모든 model의 gradient 값을 0으로 설정\n",
    "            output = model(data) # 모델을 forward pass해 결과값 저장 \n",
    "            loss = criterion(output, target) # output과 target의 loss 계산\n",
    "            loss.backward() # backward 함수를 호출해 gradient 계산\n",
    "            optimizer.step() # 모델의 학습 파라미터 갱신\n",
    "            train_loss += loss.item() * data.size(0) # loss값을 train에 더함 (나중에 loss의 평균을 구하기 위해 data.size를 곱함)\n",
    "        \n",
    "        train_loss = train_loss/len(train_loader.dataset) # 전체 train loader의 크기로 나눔\n",
    "        train_losses.append(train_loss) # 결과를 plot할때 사용하기 위해 list에 값을 append\n",
    "        \n",
    "        print('Epoch: {} \\tTraining Loss: {:.4f}'.format(\n",
    "            epoch, train_loss))     \n",
    "        \n",
    "        if train_loss <= train_loss_min: # 더 좋은 결과가 나온 경우\n",
    "            print('Train loss decreased {:.4f} --> {:.4f}. Saving model ...'.format(train_loss_min, train_loss))\n",
    "            torch.save(model.state_dict(), 'model_transfer.pth')\n",
    "            train_loss_min = train_loss # 최저 loss를 갱신함\n",
    "        \n",
    "    return model, train_losses\n",
    "\n",
    "from PIL import ImageFile \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "epochs = 30\n",
    " \n",
    "model_transfer, train_losses = train(epochs, train_batches, model_transfer, optimizer_transfer, criterion_transfer, use_cuda)\n",
    " \n",
    "model_transfer.load_state_dict((torch.load('model_transfer.pth'))) # test결과가 가장 좋았던 model을 불러와 저장\n",
    "\n",
    "# loss값 그래프 출력\n",
    "plt.plot(train_losses, label=\"Training loss\")\n",
    "plt.legend(frameon=False)\n",
    "plt.grid(True)\n",
    " \n",
    "def test(test_loader, model, criterion, use_cuda):\n",
    "    class_correct = 0.\n",
    "    class_total = 0.\n",
    "    test_loss = 0.\n",
    " \n",
    "    model.eval()\n",
    "    for data, target in test_loader:\n",
    "        if use_cuda:\n",
    "          data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)  # 모델을 forward pass해 결과값 저장 \n",
    "        loss = criterion(output, target) # output과 target의 loss 계산\n",
    "        test_loss += loss.item() * data.size(0) # loss값을 test_loss에 더함 \n",
    "        _, pred = torch.max(output, 1) # 출력이 분류 각각에 대한 값으로 나타나기 때문에, 가장 높은 값을 갖는 인덱스를 추출\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred)) # pred.eq를 사용해서 예측값과 실제값이 같은지 비교\n",
    "         # gpu tensor는 numpy로 계산할 수 없기 때문에, 조건문을 사용해 cpu에서 계산되도록 함\n",
    "        class_correct += np.sum(np.squeeze(correct_tensor.numpy())) if not use_cuda else np.sum(np.squeeze(correct_tensor.cpu().numpy()))\n",
    "        class_total += data.size(0) # 전체 클래스 개수 \n",
    " \n",
    "    test_loss = test_loss/len(test_loader.dataset) # test loss의 평균 계산\n",
    " \n",
    "    # 결과 출력\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * class_correct / class_total, class_correct, class_total))\n",
    " \n",
    "test(test_batches, model_transfer, criterion_transfer, use_cuda) # test메소드 사용\n",
    " \n",
    "class_names = [item[0:].replace(\"_\", \" \") for item in train_batches.dataset.classes] # class 목록 list를 저장\n",
    " \n",
    "train_batches.dataset.classes[:6] # class 목록 출력\n",
    " \n",
    "def image_loader(img_path, transform, use_cuda):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    img = transform(image)[:3,:,:].unsqueeze(0)\n",
    "    if use_cuda:\n",
    "        img = img.cuda()\n",
    "    return img\n",
    " \n",
    "def predict_food_transfer(model, class_names, img_path):\n",
    "    logit = model_transfer(image_loader(img_path, test_transform, True))\n",
    "    idx = torch.max(logit,1)[1].item()\n",
    " \n",
    "    return class_names[idx]\n",
    " \n",
    "dirs = os.listdir('myimg')\n",
    " \n",
    "for img_file in dirs:\n",
    "    img_path = os.path.join('myimg', img_file)\n",
    "    predition = predict_food_transfer(model_transfer, class_names, img_path)    \n",
    "    print(\"image_file_name = {0}, \\t predition food: {1}\" .format(img_path, predition))\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86ecc6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.805882\n",
      "\n",
      "\n",
      "Test Accuracy: 67% (1538/2270)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-455d4fa80786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image'"
     ]
    }
   ],
   "source": [
    "test(test_batches, model_transfer, criterion_transfer, use_cuda) # test메소드 사용\n",
    " \n",
    "class_names = [item[0:].replace(\"_\", \" \") for item in train_batches.dataset.classes] # class 목록 list를 저장\n",
    " \n",
    "train_batches.dataset.classes[:5] # class 목록 출력\n",
    " \n",
    "def image_loader(img_path, transform, use_cuda):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    img = transform(image)[:3,:,:].unsqueeze(0)\n",
    "    if use_cuda:\n",
    "        img = img.cuda()\n",
    "    return img\n",
    " \n",
    "def predict_food_transfer(model, class_names, img_path):\n",
    "    logit = model_transfer(image_loader(img_path, test_transform, True))\n",
    "    idx = torch.max(logit,1)[1].item()\n",
    " \n",
    "    return class_names[idx]\n",
    " \n",
    "dirs = os.listdir('image')\n",
    " \n",
    "for img_file in dirs:\n",
    "    img_path = os.path.join('image', img_file)\n",
    "    predition = predict_food_transfer(model_transfer, class_names, img_path)    \n",
    "    print(\"image_file_name = {0}, \\t predition: {1}\" .format(img_path, predition))\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59afec89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
